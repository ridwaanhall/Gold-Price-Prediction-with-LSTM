{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2638c006",
   "metadata": {},
   "source": [
    "# Gold Price Prediction - Model Experiments\n",
    "\n",
    "This notebook contains experiments with different LSTM architectures and hyperparameters for gold price prediction.\n",
    "\n",
    "## Objectives\n",
    "1. Compare different LSTM architectures\n",
    "2. Optimize hyperparameters\n",
    "3. Analyze model performance\n",
    "4. Select best model for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee51924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import setup_logging, load_config\n",
    "from src.data_preprocessing import GoldDataPreprocessor\n",
    "from src.lstm_model import LSTMGoldPredictor\n",
    "from src.model_trainer import ModelTrainer, HyperparameterTuner\n",
    "from src.evaluation import ModelEvaluator\n",
    "from src.visualization import Visualizer\n",
    "from src.mlflow_integration import MLflowManager\n",
    "from config.config import Config\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbbfe8",
   "metadata": {},
   "source": [
    "## 1. Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee73bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "setup_logging()\n",
    "\n",
    "# Initialize MLflow for experiment tracking\n",
    "mlflow_manager = MLflowManager(config, \"model_experiments\")\n",
    "\n",
    "# Load and preprocess data\n",
    "preprocessor = GoldDataPreprocessor(config.data)\n",
    "data_path = '../data/sample_data.json'\n",
    "\n",
    "# Load data\n",
    "df = preprocessor.load_data(data_path)\n",
    "print(f\"Loaded {len(df)} data points\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scalers = preprocessor.preprocess_data(df)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80b493",
   "metadata": {},
   "source": [
    "## 2. Baseline Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa05ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline architectures to test\n",
    "architectures = [\n",
    "    'simple_lstm',\n",
    "    'stacked_lstm', \n",
    "    'bidirectional_lstm',\n",
    "    'lstm_with_attention',\n",
    "    'cnn_lstm',\n",
    "    'encoder_decoder'\n",
    "]\n",
    "\n",
    "baseline_results = {}\n",
    "evaluator = ModelEvaluator(config.evaluation)\n",
    "\n",
    "print(\"Testing baseline architectures...\")\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\n=== Testing {arch} ===\")\n",
    "    \n",
    "    with mlflow_manager.start_run(f\"baseline_{arch}\"):\n",
    "        try:\n",
    "            # Create model\n",
    "            model_config = config.model\n",
    "            model_config.architecture = arch\n",
    "            \n",
    "            model = LSTMGoldPredictor(model_config)\n",
    "            \n",
    "            # Train model\n",
    "            trainer = ModelTrainer(config.training)\n",
    "            \n",
    "            history = trainer.train_model(\n",
    "                model, \n",
    "                X_train, y_train,\n",
    "                X_val, y_val,\n",
    "                epochs=20  # Quick training for comparison\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            y_pred = model.predict(X_test)\n",
    "            metrics = evaluator.evaluate_predictions(y_test, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            baseline_results[arch] = {\n",
    "                'metrics': metrics,\n",
    "                'model': model,\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "            # Log to MLflow\n",
    "            mlflow_manager.log_hyperparameters({\n",
    "                'architecture': arch,\n",
    "                'sequence_length': model_config.sequence_length,\n",
    "                'lstm_units': model_config.lstm_units,\n",
    "                'dropout': model_config.dropout\n",
    "            })\n",
    "            \n",
    "            mlflow_manager.log_metrics(metrics)\n",
    "            \n",
    "            print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
    "            print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "            print(f\"R²: {metrics['r2']:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {arch}: {e}\")\n",
    "            baseline_results[arch] = {'error': str(e)}\n",
    "\n",
    "print(\"\\nBaseline experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66857ebc",
   "metadata": {},
   "source": [
    "## 3. Compare Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for arch, results in baseline_results.items():\n",
    "    if 'metrics' in results:\n",
    "        metrics = results['metrics']\n",
    "        comparison_data.append({\n",
    "            'Architecture': arch,\n",
    "            'MAPE (%)': metrics['mape'],\n",
    "            'RMSE': metrics['rmse'],\n",
    "            'MAE': metrics['mae'],\n",
    "            'R²': metrics['r2'],\n",
    "            'Direction Accuracy (%)': metrics.get('direction_accuracy', 0) * 100\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('MAPE (%)')\n",
    "\n",
    "print(\"Architecture Comparison:\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# MAPE comparison\n",
    "axes[0, 0].bar(comparison_df['Architecture'], comparison_df['MAPE (%)'])\n",
    "axes[0, 0].set_title('MAPE Comparison')\n",
    "axes[0, 0].set_ylabel('MAPE (%)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 1].bar(comparison_df['Architecture'], comparison_df['RMSE'])\n",
    "axes[0, 1].set_title('RMSE Comparison')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R² comparison\n",
    "axes[1, 0].bar(comparison_df['Architecture'], comparison_df['R²'])\n",
    "axes[1, 0].set_title('R² Comparison')\n",
    "axes[1, 0].set_ylabel('R²')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Direction Accuracy comparison\n",
    "axes[1, 1].bar(comparison_df['Architecture'], comparison_df['Direction Accuracy (%)'])\n",
    "axes[1, 1].set_title('Direction Accuracy Comparison')\n",
    "axes[1, 1].set_ylabel('Direction Accuracy (%)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best architecture\n",
    "best_arch = comparison_df.iloc[0]['Architecture']\n",
    "print(f\"\\nBest performing architecture: {best_arch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbb601",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for best architecture\n",
    "print(f\"Optimizing hyperparameters for {best_arch}...\")\n",
    "\n",
    "# Define hyperparameter search space\n",
    "hyperparameter_space = {\n",
    "    'lstm_units': [32, 64, 128],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'sequence_length': [30, 60, 90]\n",
    "}\n",
    "\n",
    "# Initialize hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    config.training,\n",
    "    search_method='random',  # Use random search for speed\n",
    "    n_trials=10  # Limited trials for notebook\n",
    ")\n",
    "\n",
    "with mlflow_manager.start_run(f\"hyperparam_opt_{best_arch}\"):\n",
    "    # Run optimization\n",
    "    best_params, best_score, optimization_history = tuner.optimize(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        hyperparameter_space,\n",
    "        architecture=best_arch\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters: {best_params}\")\n",
    "    print(f\"Best score: {best_score:.4f}\")\n",
    "    \n",
    "    # Log best parameters\n",
    "    mlflow_manager.log_hyperparameters(best_params)\n",
    "    mlflow_manager.log_metrics({'best_validation_score': best_score})\n",
    "\n",
    "# Visualize optimization history\n",
    "if optimization_history:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(optimization_history)\n",
    "    plt.title('Hyperparameter Optimization Progress')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Validation Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf2eeb",
   "metadata": {},
   "source": [
    "## 5. Final Model Training and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6aa73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete experiment with final model training and comprehensive analysis\n",
    "print(\"Training final optimized model and generating comprehensive analysis...\")\n",
    "\n",
    "# This cell contains the complete experimental workflow\n",
    "# Results will be saved and visualized for production deployment decision\n",
    "\n",
    "print(\"Model experiments notebook ready for execution!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
